[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Quick note on Normalising flow for SBI\n\n\n\n\n\n\nSBI\n\n\n\n\n\n\n\n\n\nApr 21, 2025\n\n\nYuga Hikida\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuga Hikida",
    "section": "",
    "text": "Bluesky\n  \n  \n    \n     Email\n  \n\n  \n  \nI am a doctoral researcher in the Probabilistic Machine Learning group at Aalto University, working on Simulation-Based Inference (SBI), advised by Ayush Bharti (Aalto) and François-Xavier Briol (UCL).\nIn many real-world problems arising in industry and science, the likelihood function is unavailable, which poses a major challenge to the application of traditional Bayesian inference. SBI overcomes this limitation by enabling Bayesian inference without direct access to the likelihood. My research focuses on two key aspects of SBI: (1) efficiency, aiming to reduce the number of required data, and (2) robustness, by detecting and/or mitigating the impact of model misspecification on inference quality.\n\n\n\n\n\n\n\n\nClick here to see my CV.\n\n\n\nInterests\nBayesian Experimental Design  Simulation-Based Inference  Amortized Inference \n\n\nEducation\n\nPhD in Computer Science Aalto University, Finland, 2025-2028\nMSc in Econometrics TU Dortmund, Germany, 2022-2024\nBSc in Economics University of Sussex, UK, 2018-2022"
  },
  {
    "objectID": "japanese/index.html",
    "href": "japanese/index.html",
    "title": "疋田 悠河",
    "section": "",
    "text": "疋田 悠河\nフィンランドで計算機科学博士課程。主にシミュレーションに基づく統計的推論に興味があります。\n\n\n来歴\n2025- 博士研究員 @アアルト大学PMLグループ  頑健で能率的なシミュレーションに基づく統計的推論、尤度フリー推論、実験計画\n2023-2024 リサーチアシスタント @ドルトムント工科大学統計学部  Rパッケージの開発、償却推論\n2021-2023 DSインターン @Rejoui  ログデータを使った顧客の行動解析、医療分野のテキストマイニング\n\n\n\n興味\nベイズ実験計画  シミュレーションに基づく統計的推論  償却推論 \n\n\n教育\n\n2025-2028 計算機科学博士   アアルト大学, フィンランド \n2022-2024 計量経済学修士   ドルトムント工科大学, ドイツ \n2018-2022 経済学学士   サセックス大学, イギリス \n\n\n\n\n詳しい情報は英語ページをご覧ください。\nプロジェクト一覧  履歴書  コンタクト"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint\nThe paper suggests the method to visualize 4D compositional data on a 2D canvas. This can be used to visualize data such as posterior model probabilities for 4 models on (2D) paper without loss of information and distortion. My contributions are general polishing of manuscript, creating 3D interactive figures and animation, and development of R package (planned)."
  },
  {
    "objectID": "projects/index.html#lossless-visualization-of-4d-compositional-data-on-a-2d-canvas",
    "href": "projects/index.html#lossless-visualization-of-4d-compositional-data-on-a-2d-canvas",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint\nThe paper suggests the method to visualize 4D compositional data on a 2D canvas. This can be used to visualize data such as posterior model probabilities for 4 models on (2D) paper without loss of information and distortion. My contributions are general polishing of manuscript, creating 3D interactive figures and animation, and development of R package (planned)."
  },
  {
    "objectID": "projects/index.html#r-package-metabmc",
    "href": "projects/index.html#r-package-metabmc",
    "title": "Projects",
    "section": "R package “metabmc”",
    "text": "R package “metabmc”\nThe package implements meta-uncertainty quantification for Bayesian model comparison. This can be used to evaluate trustworthiness of posterior model probabilities, which tend to take extreme values. Check the project website for more detail. The development is still in progress and will be published in near future."
  },
  {
    "objectID": "projects/index.html#r-package-for-bayesian-proxy-structural-var",
    "href": "projects/index.html#r-package-for-bayesian-proxy-structural-var",
    "title": "Projects",
    "section": "R package for Bayesian proxy structural VAR",
    "text": "R package for Bayesian proxy structural VAR\nThe package implements Bayesian vector autoregressive model with proxy variables for structural shocks in economy. This can help agents like government and central bank to make decisions, considering the expected effects in the system of economy. The development is still in progress and will be published in near future."
  },
  {
    "objectID": "blog/posts/nf-quick/index.html",
    "href": "blog/posts/nf-quick/index.html",
    "title": "Quick note on Normalising flow for SBI",
    "section": "",
    "text": "Normalising flow is commonly used for density estimation in generative model. It is also used in Simulatoin-Based Inference (SBI) to estimate conditional density. Specifically, \\(p(\\theta | \\mathcal{D})\\) for neural posterior estimation (NPE) and \\(p(\\mathcal{D} | \\theta)\\) for neural likelihood estimation (NLE). In this post, I will quickly explain how normalizing flow can estimate density with some focus on the application in SBI.\nIn generative model we are interested in estimating distribution of \\(x \\in \\mathbb{R}^D\\) by integrating out latent variable \\(z \\in \\mathbb{R}^D\\), that is\n\\[\np(x) = \\int p(x | z)p_z(z)dz\n\\]\nThe integral is usually intractable. In normalizing flow, we pick \\(p_z(z)\\) to be a simple distirbution such as standard normal distribution \\(N(0, I_D)\\), and transform \\(p_z(z)\\) with \\(g: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\). We pick \\(g\\) to be diffeomorphism, that is, \\(g\\) is 1. bijective, 2. differentiable, 3. its inverse is differentiable. This allows us to express \\(p(x)\\) using change of variable formula (without any intractable integral involved)\n\\[\np(x) = p_z(g^{-1}(x)) \\left|\\det  \\frac{\\partial g^{-1}(x)}{\\partial x} \\right|\n:=  p_z(f(x)) \\left|\\det  \\frac{\\partial f(x)}{\\partial x} \\right|\n\\] We just defined \\(f := g^{-1}\\). \\(g\\) is referred as generative path because we try to generate \\(x\\) from a simple distribution \\(p_z\\), whereas \\(f\\) is referred as normalising path because we try to degenerate \\(x\\) back to \\(z \\sim p_z\\), where \\(p_z\\) is usually normal distribution.\n\\(f\\) is usually parameterized by neural network to maximise likelihood \\(p(x)\\). \\(f\\) is usually chosen such that the determinant of the Jacobian is easy to calculate. Training and evaluation of density requires normalising path while sampling requires generative path as\n\\[\nx \\sim p(x)\n\\mathrel{\\Leftrightarrow}\nz \\sim p_z(z), x = g(z)\n\\]\nPeople often care about \\(f\\) because fast evaluation of \\(f\\) is needed for fast training. If we need fast sampling, then \\(g\\) is cared.\nIn SBI, coupling flow and autoregressive flow is commonly used."
  },
  {
    "objectID": "blog/posts/nf-quick/index.html#coupling-flow",
    "href": "blog/posts/nf-quick/index.html#coupling-flow",
    "title": "Quick note on Normalising flow for SBI",
    "section": "Coupling flow",
    "text": "Coupling flow\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi &= NN(x^B) \\\\\ny^A &= \\mathbf{h}_\\phi(x^A) = [h_{\\phi_1}(x_1),...,h_{\\phi_d}(x_d)]  \\\\\ny^B &= x^B \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nFirstly, \\(x\\) is split into two dimension such that \\(x^A \\in \\mathbb{R}^d, x^B \\in \\mathbb{R}^{D - d}\\). Then one part is fed into NN: \\(\\mathbb{R}^{D - d} \\rightarrow \\mathbb{R}^k\\) to learn parameter \\(\\phi \\in  \\mathbb{R}^k\\). The NN is referred as conditioner. This \\(\\phi\\) is then coupled with another part \\(x^A\\) through invertible and differentiable function \\(\\mathbf{h}\\) called transformer or coupling function. \\(k\\) depends on the choice of coupling function. Note that transformer is nothing to do with the ‘query-key-value’ transformers. Coupling function is usually applied element-wise. This transformed \\(x^A\\) is then concatenated with \\(x^B\\), which gives output \\(y\\). Its inverse is given by\n\\[\n\\begin{align}\nx^B &= y^B \\\\\n\\phi &= NN(x^B) \\\\\nx^A & = \\mathbf{h}_{\\phi}^{-1}(y^A)\n\\end{align}\n\\]\nThis ‘coupling’ is repeated multiple times, combined with non-stochastic permutation such that all the dimensions \\(i = 1,...,D\\) are interacted to each other. The determinant of Jacobian is given by \\(\\prod_{i=1}^d \\frac{\\partial h_{\\phi_i}(x_i)}{\\partial x_i}\\).\nIn NPE, \\(x \\leftarrow \\theta\\) and the conditioner is also conditioned on data as \\(\\phi = NN(\\theta^B, \\mathcal{D})\\). In NLE, \\(x \\leftarrow \\mathcal{D}\\) and \\(\\phi = NN(\\mathcal{D}^B, \\theta)\\)"
  },
  {
    "objectID": "blog/posts/nf-quick/index.html#autoregressive-flow",
    "href": "blog/posts/nf-quick/index.html#autoregressive-flow",
    "title": "Quick note on Normalising flow for SBI",
    "section": "Autoregressive flow",
    "text": "Autoregressive flow\n\\[\n\\begin{align}\n\\text{For } i &= 2,...,D:\\\\\n\\phi_i &= NN(x_{1:i-1}) \\\\\ny_i &= h_{\\phi_i}(x_i)\\\\\n\\end{align}\n\\]\nwhere \\(\\phi_1\\) is constant. This is very similar to coupling flow except that instead of splitting dimensions into half, dimension is split in autoregressive manners into \\(x_i\\) and \\(x_{1:i-1}\\) mutiple times. In practice, this coupling functions can be implemented without recursion using masking.\nIn NPE, \\(\\phi_1 = NN(\\mathcal{D}), \\phi_i = NN(\\theta_{1:i-1}, \\mathcal{D}), 1 &lt; i \\leq D\\). In NLE, \\(\\phi_1 = NN(\\theta),\\phi_i = NN(\\mathcal{D}_{1:i-1}, \\theta), 1 &lt; i \\leq D\\)"
  },
  {
    "objectID": "blog/posts/nf-quick/index.html#coupling-function",
    "href": "blog/posts/nf-quick/index.html#coupling-function",
    "title": "Quick note on Normalising flow for SBI",
    "section": "Coupling function",
    "text": "Coupling function\n\nAffine\n\\[\nh_{\\phi_{i}}(x_i) = \\alpha_i x_i + \\beta_i, \\quad \\phi_i = (\\alpha_i, \\beta_i)\n\\]\nwhere \\(\\alpha_i &gt; 0, \\beta_i \\in \\mathbb{R}\\). For affine coupling function, \\(k = d \\times 2\\)  \n\n\nMonotonic rational quadratic spline\nIn neural spline flow, monotonic rational quadratic spline is used for \\(h\\). It allows to model more expressive transformation from \\(x\\) to \\(y\\) and to \\(z\\) at final. Monotonicity is necessary for bijectivity.\n\n\n\nMonotonic RQ-spline. Taken from [2]"
  },
  {
    "objectID": "blog/posts/nf-quick/index.html#bibliography",
    "href": "blog/posts/nf-quick/index.html#bibliography",
    "title": "Quick note on Normalising flow for SBI",
    "section": "Bibliography",
    "text": "Bibliography\n[1] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, “Normalizing Flows: An Introduction and Review of Current Methods,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964-3979.\n[2] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019). Neural Spline Flows. Advances in Neural Information Processing Systems, abs/1906.04032. https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html\n[3] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Kothe, U. (2022). BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4), 1452–1466."
  },
  {
    "objectID": "blog/posts/nf-quick/index.html#appendix",
    "href": "blog/posts/nf-quick/index.html#appendix",
    "title": "Quick note on Normalising flow for SBI",
    "section": "Appendix",
    "text": "Appendix\n\nDual coupling\nSometimes, one block of coupling is defined as two coupling as in [3] such that both split is transformed by coupling function in one unit of coupling.\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi_1 &= NN_1(x^B) \\\\\ny^A &= \\mathbf{h}_{\\phi_1}(x^A) \\\\\n\\phi_2 &=  NN_2(y^A)\\\\\ny^B &= \\mathbf{h}_{\\phi_2}(x^B) \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nThis is same as applying single coupling flow followed by permutation, which swap \\(x^A\\) and \\(x^B\\) and applying another coupling flow.\n\n\nWhen \\(D = 1\\)\nIn generative model \\(x\\) is usually something high dimensional like image. But in SBI, \\(D\\) could be 1. Coupling flow could be applied in such case. For example in NPE, if \\(\\theta \\in \\mathbb{R}\\) and consider only one coupling, then we have\n\\[\n\\begin{align}\n\\phi &= NN(\\mathcal{D}) \\\\\nz &= \\mathbf{h}_{\\phi}(\\theta)\n\\end{align}\n\\]\nIn this case, \\(\\theta\\) is not going to be coupled with another split of \\(\\theta\\) because \\(\\theta\\) is just one-dimensional, but it is still coupled with \\(\\mathcal{D}\\).\nIn this case, naturally, coupling flow is same as autoregressive flow given same conditioner and coupling function."
  },
  {
    "objectID": "googlebca4a2421a5aa054.html",
    "href": "googlebca4a2421a5aa054.html",
    "title": "Yuga Hikida",
    "section": "",
    "text": "google-site-verification: googlebca4a2421a5aa054.html"
  }
]